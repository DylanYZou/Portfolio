{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4351714b",
   "metadata": {},
   "source": [
    "# Homework Assignment 1 - Parametric Models and Gradient Descent\n",
    "### **Due:** Thursday, Apr 20, 11:59pm\n",
    "### Total: 100 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ff3df",
   "metadata": {},
   "source": [
    "## **1)** Deriving the Cross-entropy Cost (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d27630",
   "metadata": {},
   "source": [
    "Derive the binary cross-entropy cost for Logistic Regression, following a similar logic with the derivation of the MSE cost for Linear Regression. Note that:\n",
    "\n",
    "- First assumption is that the target values - $y^{(i)}s$ - come from a Bernoulli distribution.\n",
    "- Second assumption is that all the instances in the training dataset are independent.\n",
    "- Based on the two assumptions, you need to write the likelihood and loglikelihood functions you need to maximize for a trained Logistic Regression model.\n",
    "\n",
    "You can submit your derivation in any format, as long as it is legible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3936d",
   "metadata": {},
   "source": [
    "Step 1: \n",
    "- Assume that the binary cross entrophy is a bernoulli where P(y|x) is 1 if y = 1, and 0 if y = 0.\n",
    "- logistic regression unknown probability function is modelled as P(y|x, w) = 1/1+e^(-(w^t)*x))\n",
    "\n",
    "Step 2. \n",
    "- Assuming independence, which means we can equate several functions to each other, such as P(y|x) to an ultimate formula.\n",
    "- Using the likelihood function L(w) = P(y_i|x_i ; w) multiplied together m times starting at i = 1, and i is 1, 2, 3... where w is the parameters that maximizes L(w)\n",
    "- we then negative log both sides (-Log(L(w) = ...), to get P(y|x, w).\n",
    "- By the fact P(0|x; w) = 1 - P(1|x; w), which means via exponentiation we can write that P(y|x; w) = P(1|x; w)^y x P(0|x; w)^1-y\n",
    "\n",
    "Step 3.\n",
    "- Plugging this expression back into negative log-likelihood, J(w) = (-1/m)/Sum of y_i*log(P(1|x_i, w) + (1 - y_i)log(1 - P(0|x_i, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7524f49",
   "metadata": {},
   "source": [
    "## 2) Implementing Logistic Regression with Gradient Descent (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6796c",
   "metadata": {},
   "source": [
    "In this question, you need to implement a Logistic Regression model from scratch It should use the training dataset for the Gradient Descent algorithm and then, use the optimum parameters with the test data to return the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730ce86",
   "metadata": {},
   "source": [
    "**a)** Import all the libraries and tools you need below. **Note that you can use the helper functions and scalers of scikit-learn but you cannot use the models.** **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba169270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir(\"C:\\Data\")\n",
    "os.getcwd()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy import log,dot,e,shape\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ac392",
   "metadata": {},
   "source": [
    "**b)** Upload the **heart_disease_classification.csv** file. Split the features and the target column into different variables. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5214218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('heart_disease_classification.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3d019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = 'target')\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30e959",
   "metadata": {},
   "source": [
    "**c)** Find the three columns that are categorical and non-binary. Create binary columns from them. **(4 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cde2997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "sex           int64\n",
       "cp            int64\n",
       "trestbps      int64\n",
       "chol          int64\n",
       "fbs           int64\n",
       "restecg       int64\n",
       "thalach       int64\n",
       "exang         int64\n",
       "oldpeak     float64\n",
       "slope         int64\n",
       "ca            int64\n",
       "thal          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeb23542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['cp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b0e387e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['slope'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c324ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['restecg'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "215e9ed2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['ca'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab16605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['thal'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f201baf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8603b31f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp/ipykernel_17612/2855706988.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['thal'][i] = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "        if data['thal'].iloc[i] == 2 or 3 or 4:\n",
    "            data['thal'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30235b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp/ipykernel_17612/254916671.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['ca'][i] = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "        if data['ca'].iloc[i] == 2 or 3 or 4:\n",
    "            data['ca'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f119a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp/ipykernel_17612/1777719670.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['slope'][i] = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "        if data['slope'].iloc[i] == 2 or 3 or 4:\n",
    "            data['slope'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "377745ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp/ipykernel_17612/1971443818.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['restecg'][i] = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "        if data['restecg'].iloc[i] == 2 or 3 or 4:\n",
    "            data['restecg'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbb11af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp/ipykernel_17612/3752776089.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cp'][i] = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "        if data['cp'].iloc[i] == 2 or 3 or 4:\n",
    "            data['cp'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb0cfaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age          77.0\n",
       "sex           1.0\n",
       "cp            1.0\n",
       "trestbps    200.0\n",
       "chol        564.0\n",
       "fbs           1.0\n",
       "restecg       1.0\n",
       "thalach     202.0\n",
       "exang         1.0\n",
       "oldpeak       6.2\n",
       "slope         1.0\n",
       "ca            1.0\n",
       "thal          1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(columns = 'target')\n",
    "Y = data['target']\n",
    "\n",
    "X.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e47657",
   "metadata": {},
   "source": [
    "**d)** Split the data into training and test datasets with a 80-20 split. Use **random_state=42**. Then, scale the features of both datasets. **(4 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dec1000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, \n",
    "                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ba1c7",
   "metadata": {},
   "source": [
    "**e)** Write the shorter helper functions first: **initialize** **(1 point)**, **predict** **(5 points)** and **sigmoid**. **(5 points)**\n",
    "\n",
    "- **initialize** can be the same function you wrote in your in-class assignment.\n",
    "- **predict** should be similar to the function you wrote in your in-class assignment as well. Here, however, it should take one more input, **threshold**. It should take the model parameters and the test data as other inputs and return class predictions using the Logistic Regression formula and the input threshold.\n",
    "- **sigmoid** should take one input and return its sigmoid value, between 0 and 1. Note that you did not need this function for Linear Regression but you need to use it in multiple places for Logistic Regression.\n",
    "\n",
    "**Make sure you use the numpy library tools for anything numeric, so that the helper functions work both for vectors and scalar values.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "229a394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dimension):\n",
    "    weights = np.full((dimension,),0.01)\n",
    "    bias = 0.0\n",
    "    return weights,bias\n",
    "\n",
    "def sigmoid(z):\n",
    "        sig = 1/(1+e**(-z))\n",
    "        return sig\n",
    "    \n",
    "def predict(model_parameters, x_test, threshold):\n",
    "    weights = model_parameters['weights']\n",
    "    bias = model_parameters['bias']\n",
    "    y_pred = sigmoid(np.dot(x_test, weights) + bias)\n",
    "    return (y_pred >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf3165",
   "metadata": {},
   "source": [
    "**f)** Write the **calculate_cost_gradient** function. Note that it can be similar to the function you wrote for linear regression, however the cost and its gradients are different for logistic regression. You can keep the format of inputs and outputs the same. **(10 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f919ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(weights, bias, x_train, y_train):\n",
    "        shape = x_train.shape[0]\n",
    "        \n",
    "        z = np.dot(x_train,weights)\n",
    "\n",
    "        cost0 = sigmoid(z)\n",
    "            \n",
    "        #calculating gradient?\n",
    "        bias = (1/shape) * np.sum(cost0-y_train)\n",
    "        weight = (1/shape) * np.dot(x_train.T, cost0-y_train)\n",
    "        cost = (-1 / shape) * np.sum(y_train * np.log(cost0) \n",
    "                                     + (1 - y_train) * np.log(1 - cost0))\n",
    "        \n",
    "        gradient = {'weight_gradient':weight, 'bias_gradient':bias}\n",
    "        return cost,gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367da182",
   "metadata": {},
   "source": [
    "**g)** Write the **GradientDescent** function. **(10 points)** You can use the function for linear regression, however, for this assignment, you need to add one more input: **tol**. \n",
    "- This new input should be a tolerance value that should be compared with the difference between current and previous cost at every iteration. \n",
    "- If the difference is smaller than the tolerance, the iteration should stop and the parameters should not be updated anymore.\n",
    "- If the maximum number of iterations is reached and the difference is still above the tolerance, the function should print a warning text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ffa275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(weights,bias, x_train, y_train, learning_rate, iterations, tol):\n",
    "    costList = []\n",
    "    index = []\n",
    "    \n",
    "    for i in range(0, iterations):\n",
    "        # For the given weights and bias, get the cost and the gradient...just call the calculate cost gradient\n",
    "        # function from above\n",
    "        cost = calculate_cost_gradient(weights, bias, \n",
    "                                       x_train, y_train)[0]\n",
    "        gradient = calculate_cost_gradient(weights ,bias,\n",
    "                                           x_train,y_train)[1]\n",
    "        \n",
    "        # update the weights and the bias\n",
    "        weights = weights - (learning_rate * gradient['weight_gradient'])\n",
    "        bias = bias - (learning_rate * gradient['bias_gradient'])\n",
    "        \n",
    "        costList.append(cost)\n",
    "        index.append(i)\n",
    "        \n",
    "        if len(costList) >= 2:\n",
    "            cost_diff = costList[i]-costList[i-1]\n",
    "        \n",
    "            if cost_diff < tol:\n",
    "                break\n",
    "                \n",
    "        if i == iterations-1:\n",
    "            print('WARNING MAX ITERATIONS REACHED')\n",
    "    \n",
    "    optimum_params = {'weights':weights, 'bias':bias}\n",
    "    print('Total iterations:', i)\n",
    "    print('Final MSE cost:', cost)\n",
    "    \n",
    "    plt.plot(index, costList)\n",
    "    plt.xlabel('number of iterations')\n",
    "    plt.ylabel('MSE cost')\n",
    "    plt.show()\n",
    "    \n",
    "    return optimum_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f9506",
   "metadata": {},
   "source": [
    "**h)** Lastly, using all the helper function above, write the **logistic_regression** function. **(10 points)**\n",
    "\n",
    "- It should take eight inputs: **x_train**, **y_train**, **x_test**, **y_test**, **learning_rate**, **num_iterations**, **tol** and **threshold**. **threshold** should take a default value of 0.5.\n",
    "\n",
    "- It should return one output: **optimum_parameters**\n",
    "\n",
    "- It should print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4dd75fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations, tol, threshold=0.5):\n",
    "    weights, bias = initialize(x_train.shape[1])\n",
    "    optimum_params = GradientDescent(weights, bias, x_train, y_train, \n",
    "                                     learning_rate, num_iterations, tol)\n",
    "    y_pred = predict(optimum_params, x_test, threshold)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print('Accuracy:', accuracy)\n",
    "\n",
    "    return optimum_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715998c",
   "metadata": {},
   "source": [
    "**i)** Run the **logistic_regression** function. Pick the **learning_rate**, **num_iterations** and **threshold** values you think are right, as long as the model converges. You can use the default threshold value of 0.5 Note that you can compare your results with the scikit-learn model with **penalty=None**. **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "beba13b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp/ipykernel_17612/4034863568.py:7: RuntimeWarning: overflow encountered in power\n",
      "  sig = 1/(1+e**(-z))\n",
      "C:\\Users\\dylan\\AppData\\Local\\Temp/ipykernel_17612/3653958108.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = (-1 / shape) * np.sum(y_train * np.log(cost0)\n",
      "C:\\Users\\dylan\\AppData\\Local\\Temp/ipykernel_17612/3653958108.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  + (1 - y_train) * np.log(1 - cost0))\n",
      "C:\\Users\\dylan\\AppData\\Local\\Temp/ipykernel_17612/888626284.py:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cost_diff = costList[i]-costList[i-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING MAX ITERATIONS REACHED\n",
      "Total iterations: 999\n",
      "Final MSE cost: inf\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5573770491803278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'weights': array([-31.45018314,  -7.99519111,  -0.51337232, -31.37446526,\n",
       "          1.79154505,   0.67606949,  -0.51337232,  53.77975435,\n",
       "         -8.05349668, -19.4072177 ,  -0.51337232,  -0.51337232,\n",
       "         -0.51337232]),\n",
       " 'bias': -0.5233723227621006}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(x_train, y_train, x_test, \n",
    "                    y_test, 0.1, 1000, 0.000001, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af4b25",
   "metadata": {},
   "source": [
    "## 3) Adding Ridge Penalty (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0930c",
   "metadata": {},
   "source": [
    "You need to edit one of the helper functions above to introduce Ridge penalty to the implementation. Find that function, copy it below and add the necessary changes. Note that lambda needs to be an additional input to the function.\n",
    "\n",
    "You don't have to run the new function with everything else above but it would be a good way to check if it is working. Note that if you do it, you need to change a few other things in the entire implementation, so copying everything to a new cell without altering your answer above is strongly suggested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d844c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(weights,bias,x_train,y_train, lamb):\n",
    "    shape = x_train.shape[0]\n",
    "    z = np.dot(x_train, weights) + bias\n",
    "    cost0 = sigmoid(z)\n",
    "    \n",
    "    \n",
    "    cost = (-1 / shape) * np.sum(y_train * np.log(cost0) \n",
    "                                 + (1 - y_train) * np.log(1 - cost0)) + (lamb / (2 * shape)) * np.sum(np.square(weights))\n",
    "    #calculate the gradient\n",
    "    bias = (1 / shape) * np.sum(cost0-y_train)\n",
    "    weight = (1 / shape) * np.dot(x_train.T, cost0-y_train) \n",
    "        + (lambd / shape) * weights\n",
    "    \n",
    "    gradient = {'weight_gradient':weight, 'bias_gradient':bias}\n",
    "    return cost, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68a640",
   "metadata": {},
   "source": [
    "## **4)** Final Questions (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667cec4",
   "metadata": {},
   "source": [
    "You may need to do some research for some of these questions.\n",
    "\n",
    "**a)** What would be the problem with adding Lasso penalty instead of Ridge above? (Different software developers have different numerical solutions for this problem.)\n",
    "\n",
    "**b)** Is there a model that includes both Ridge and Lasso penalty? What is it called? How many hyperparameters does it have?\n",
    "\n",
    "**c)** If there are off-the-shelf scikit-learn models available, what is the point of implementing Linear/Logistic Regression and Gradient Descent from scratch?\n",
    "\n",
    "**(5 points each)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5fc61",
   "metadata": {},
   "source": [
    "a. because lasso uses absolute value of estimated co-efficients via L1 regularization, which would not be compliant with the threshold introduced\n",
    "\n",
    "b. Elastic-Net Regression and it contains 2 hyperparameters\n",
    "\n",
    "c. because then it makes it customizable between the inputs. it also allwos one to delve into the mechanics and gears of the logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
